\chapter{Cosmological Simulations}

full approximation scheme \parencite[FAS, see, e.g.,][]{MG_overview}

\todo{Notation \(n,N,L,m\)

}

\section{\nbody\ problem}
Many problems in physics, including cosmology, involve particle systems with each particle interacting with all other particles present. In astronomy there is a gravitational interaction between starts, galaxies and cluster of galaxies, depending on scale which we are studying. The challenge of efficiently carrying out the related calculations is generally known as the \nbody\ problem.

The main problem is following -- we have $N_p$ particles all interacting gravitationally with each other. To compute a trajectory of even a single particle involves computing trajectories of all other particles as the gravitational force is dependent on time-varying positions of other particles. That means that at each time-step we have to compute force from all other $N_p$ particles and we need to compute these forces for each of $N_p$ particles. This leaves us with complexity of \(\OO(N_p^2)\). This brute force approach can be used only for small systems and is computationally unimaginable for large systems in cosmology which typically involve \(N_p\sim10^{9}\) particles.

This direct approach is generally referred to as the Particle-Particle (PP) method \parencite{Hockney:1988:CSU:62815}. Although computationally expensive the accuracy in the force calculation is of machine precision. To be able to simulate large systems of particles we need to drop the accuracy of continuous positions and use discrete positions for force calculations. In our code we use two main methods for force calculations -- the Particle-Mesh algorithm (PM) and grid-based methods, both of them we describe in more details below.
\subsection{Time integration}
The accurate time integration is a very important part of any \nbodysim. While there are many different methods to integrate particle trajectories \parencite[see e.g.][]{Hockney:1988:CSU:62815} we describe here one of the most used one in collisionless simulations -- the Leapfrog integrator.

The leapfrog integrator is an example of a symplectic integrator. Symplectic integrators exactly solve an approximate Hamiltonian. As a consequence, the numerical time evolution is a canonical map and preserves certain conserved quantities exactly, such as the total angular momentum, the phase-space volume, and the Jacobi constants. The idea is to approximate the Hamiltonian \(H\) governing motion of particles with an approximat one
\eq{
    \tilde H=H+H\err\,,
}
where \(H\err\) is the error Hamiltonian. Provided that \(\tilde H\) and \(H\) are time-invariant, the energy error is bounded at all times. The goal now is to find \(\tilde H\) that can be solved exactly by simple numerical means and minimises \(H\err\). Defining the combined phase-space coordinates \(w\equiv(x,p)\) the Hamilton’s equations are
\eq{
    \dot w=\HH w\,,
}
where \(\HH\equiv\{\cdot,H\}\) is an operator acting on \(w\) through Poisson bracket. Hamilton’s equations have then the formal solution
\eq{
    w(t+\Delta t)=e^{\Delta t\HH}w(t)\,.
}
\begin{sloppypar}
The operator \(e^{\Delta t\HH}\) can be split, in an approximate sense, into a succession of discrete but symplectic steps, each of which can be exactly integrated. The most common choice is to separate out the kinetic and potential energies, \({H=T(p)+V(x)}\), such that we can split
\end{sloppypar}
\eq{
    \label{eq:kdk}
    e^{\Delta t\HH}= e^{\Delta t(\TT+\VV)}\approx e^{\frac12\Delta t\VV}e^{\Delta t\TT}e^{\frac12\Delta t\VV}\,,
}
where operators \(\TT\equiv\{\cdot,T\}\) and \(\VV\equiv\{\cdot,V\}\) are known as drift and kick, as they only change either the positions (drift) or velocities (kick). Because these operators are non-commutative, the central relation in \eqref{eq:kdk} is only approximately true. This operator splitting is extremely useful, because the new equations have simple solution:
\eq{
    e^{\Delta t\TT}
    \begin{pmatrix}
        x \\
        p \\
    \end{pmatrix}=
    \begin{pmatrix}
        x+\Delta t\ p \\
        p \\
    \end{pmatrix}
    \mspace{20mu}\rm{and}\mspace{20mu}
    e^{\Delta t\VV}
    \begin{pmatrix}
        x \\
        p \\
    \end{pmatrix}=
    \begin{pmatrix}
        x \\
        p-\Delta t\ \nabla V \\
    \end{pmatrix}\,.
}
The splitting \eqref{eq:kdk}, also known as kick-drift-kick (KDK), is second order accurate, whereas simpler splitting only into one kick and one drift is first order accurate. The same accurcy and results has also similar splitting into drift-kick-drift which we use in our integrator.
\section{Particle-Mesh algorithm}
As we mentioned in the introduction in order to effectively compute forces on particles one has to abandon continuous position and solve the forces using discretized positions. The idea of the PM method is that we set up a mesh (grid) over the computational box, and then solve the gravitational potential (i.e. Poisson’s equation \eqref{eq:poisson_lin}) at the meshpoints. Forces at the meshpoints can then be obtained by calculating the gradient of the potential. The four principal steps of the particle-mesh calculation are:
\begin{enumerate}
    \item assign mass to the mesh,
    \label{it:pm_1}
    \item solve the field equation on the mesh,
    \label{it:pm_2}
    \item calculate the mesh-defined force field,
    \label{it:pm_3}
    \item interpolate to find forces on the particles,
    \label{it:pm_4}
\end{enumerate}
where the forces found at the last step are used to integrate the equations of motion.
\subsection{Assignment schemes}
In step \ref{it:pm_1} and \ref{it:pm_4} we make a connection between discrete mesh and continuous positions of the particles. In the first we need to distribute mass of all particles onto the mesh and in the last step, on the contrary, we need to interpolate forces known on the discrete mesh onto particles with continuous positions. There are several ways of assigning the particle masses to the discrete density function $\delta$ and discrete force $-\nabla\Phi$ to the particles:

\subsubsection{Nearest gridpoint (NGP)}
The mass of each particle $m_i$ is assigned as a whole to the gridpoint closest to the particle. Similary, force on particles is given by force on the nearest gridpoint. Although computationally the simplest, NGP as a zero-order interpolation offer lowest accuracy and the interparticle force changes discontinuously as particles cross cell boundaries.

\subsubsection{Cloud-in-Cell (CIC)}
The mass of each particle is weighted over the eight closest cells while the the weighting in each dimension is proportional to the distance between particle and the mesh. CIC scheme is more costly in terms of number of arithmetic operations per particle per timestep than NGP but offers better accuracy as first order (linear) interpolation. The CIC interpolation function in one dimension is
\eq{
    w(x-x_p)=
    \begin{cases}
        1-\frac{|x-x_p|}{H} & |x-x_p|<H\,\\
        0 & \rm{otherwise}
    \end{cases}
}
where $x$ is the position of a mesh point, $x_p$ position of the partice and $H$ the distance between mesh points. The three-dimensional interpolation function is \(W(\mathbf x)=w(x_1)w(x_2)w(x_3)\). The CIC scheme is somewhere in the middle between oversimplistic NGP and higher order interpolations which offer greater accuracy and smoother transitions but at the cost of much greater computational resources -- second order interpolation (triangular shaped cloud, TSC) involves 27 cells, third order 64, and etc. CIC gives a much smoother force than the NGP scheme (piecewise linear) and reduces the amplitude of fluctuations in the interparticle forces as the particles are displaced with respect to the mesh. In our code we use the CIC scheme.
\subsubsection{Mixed schemes}
The NGP and CIC schemes both employ mass assignment functions which are the same as their force interpolation functions. A result of this symmetry is that they conserve momentum, i.e., the forces between a pair of particles are equal and opposite, and the force of a particle upon itself (the self force) is zero. A possible variation is to use a mass assignment function which is different from the force interpolation function. In such cases, symmetry still causes the force between pairs of particles to be equal and opposite, but no longer ensures that the self force is zero. At best, the presence of the self force presents a nonphysical restriction on the timestep and, at worst, it is disastrous.

\subsection{Solution of the field equations}
An efficient method for the solution of the field equations is a necessary requirement for the practical implementation of the particle simulation algorithms that have been described. For linear Poisson’s equation \eqref{eq:poisson_lin} the most used method is the Fast Fourier Transformation whcih can compute convolutions involve in the force computation rapidly with complexity of $\OO(N_m\log N_m)$. The FFT also automatically solves periodic boundary conditions usually incorporated in cosmological simulations to enforce homogeneity and isotropy of the Universe. Therefore, FFT-based methods are predominantly used with cosmological simulations.

The Green's function $G(k)$ of \eqref{eq:poisson_lin} is not the expected one, $-1/k^2$, but we have the freedom to choose such a Green`s function which will minimize errors introduce by all the steps involve in force calculation -- mass assignment, potential solver, finite difference and force interpolation. This topic is in great detail studied in \textcite{Hockney:1988:CSU:62815}.
\subsection{Particle-Particle Particle-Mesh algorithm}
The PM methods are excellent for solving long-range forces between particles as these methods are very fast and accurate. However, this accuracy is limited by the mesh size $H$ and PM algorithms cannot truly describe anything below this scale. The accuracy is given by the Nyquist wavelength $\lambda_N=2H$, which is the shortest spatial wavelength that can be accurately recovered. Similar, power on wavenumbers larger than the Nyquist wavenumber $k_N=2\pi/\lambda_N=N_m\pi/L$ will be aliased into longer waves. To overcome this limitations Particle-Particle Particle Mesh algorithms (P$^3$M) have neen proposed \textcite{hockney_10000_1973}.

The essence of the method is to express the interparticle force as the sum of two component parts; the short-range part, which is nonzero only for particle separations less than some cutoff radius, and the smoothly varying long-range part, which has a transform which is approximately band-limited (that is to say, is approximately nonzero for only a limited range of $k$). The total short-range force on a particle is computed by direct particle-particle (PP) pair force summation and the smoothly varying part is approximated by the particle-mesh (PM) force calculation.

We do not use P$^3$M in our approximate methods because of their much greater numerical requirements compared to simpler PM methods. For more details, see e.g. \textcite{Hockney:1988:CSU:62815}.
\section{Multi-grid techniques}
Practical multi-grid techniques were introduced by \textcite{10.2307/2006422}. These methods also interpolate the density and potential between grid and particles as PM methods based on FFT with $\OO(N_m\log N_m)$ complexity but they use relaxation methods such as Gauss-Seidel iteration \parencite{doi:10.1002/zamm.19720520813} and can solve elliptic partial differential equations using $\OO(N_m)$ operations. The numerical coefficients in these estimates are such that multigrid methods are comparable to the rapid methods in execution speed. Unlike the rapid methods, however, the multigrid methods can solve general elliptic equations with nonconstant coefficients with hardly any loss in efficiency. Even nonlinear equations of modified gravitational can be solved with these methods \parencite{10.5555/42249}.

The basic idea exploits the fact that on a coarser grid relaxation occurs faster because information travels faster. The distribution of errors (the difference between the actual density -- source -- and that obtained via the discretisation of potential from the current estimate for the potential) is first smoothed on the finest grid by a few Gauss-Seidel iterations. After transferring the problem to a coarser grid, the process is repeated on coarser and coarser grids until, on the coarsest grid convergence is achieved. Then the problem is transferred back to finer and finer grids, each time iterating until convergence.
\subsection{Simple two-grid V-cycle}
The key idea of the multigrid method can be understood by considering the simplest case of a two-grid method. Suppose we are trying to solve the discretized linear elliptic problem on a uniform grid with mesh size $h$
\eq{
    \LL_h u_h=f_h\,,
}
where $\LL$ is some linear elliptic operator and $f$ is the source. The error or correction of the (exact) solution $u_h$ to some approximate solution $\tilde u_h$ is
\eq{
    v_h\equiv u_h-\tilde u_h.
}
The residual or defect is
\eq{
    \label{eq:defect}
    d_h\equiv\LL \tilde u_h-f_h\,.
}
Since $\LL_h$ is linear, the error satisfies
\eq{
    \label{eq:residual}
    \LL_hv_h=-d_h\,.
}
In order to find $v_h$ we need to make an approximation to $\LL_h$. This can be done using classical iteration methods, such as Jacobi or Gauss-Seidel, which use simpler operator than $\LL_h$, e.g. only the diagonal part (Jacobi iteration) or the lower triangle (Gauss-Seidel iteration).

Instead of using simpler form of $\LL_h$ we can use coarser form, i.e. we will solve the problem using $\LL_H$ on a coarser grid with $H=2h$. The residual equation \eqref{eq:residual} is now approximated by
\eq{
    \label{eq:residual_H}
    \LL_Hv_H=-d_H\,.
}
Since $\LL_H$ has smaller dimension this equation can be solved much faster than the original fine-grid solution. To transform residuals from fine to coarse grid we need a restriction operator $\RRR$, and similarly we need a  prolongation operator $\PP$ to interpolate corrections from coarse to fine grid.
\eq{
    \label{eq:restrict}
    d_H &= \RRR d_h\,,\\
    \label{eq:prolong}
    \tilde v_h &= \PP \tilde v_H\,,
}
where $\tilde v_H$ is some approximate solution of \eqref{eq:residual_H}. The new approximation $\tilde u_h\new$ is then obtained as
\eq{
    \label{eq:new_app}
    \tilde u_h\new = \tilde u_h + \tilde v_h\,.
}

The idea behind multigrid techniques is a combination of smoothing on the fine grid (relaxation) and on coarse grid (coarse-grid correction). Relaxation can rapidly reduce high-frequency errors but low-frequency errors tend to converge slowly. On the other hand, high frequency errors are not even representable on the coarse grid and low-frequency errors converge much quickly. The whole scheme, two-grid V-cycle, is thus:
\begin{itemize}
    \item Apply a relaxation method to $\tilde u_h$ (pre-smoothing),
    \item Compute the defect on the fine grid from \eqref{eq:defect},
    \item Restrict the defect by \eqref{eq:restrict},
    \item Solve \eqref{eq:residual_H} on the coarse grid,
    \item Interpolate the correction to the fine grid by \eqref{eq:prolong},
    \item Compute the next approximation by \eqref{eq:new_app},
    \item Apply a relaxation method to $\tilde u_h\new$ (post-smoothing).
\end{itemize}
To achieve the full multigrid method is now straightforward. Instead of solving the coarse-grid defect exactly we can get an approximate solution of it by introducing an even coarser grid and so on.

We considered $\LL$ to be a linear operator but the above method works also with non-linear operators. The speed of convergence is obviously worse and depend on the exact form of the operator. For our problem and finding solutions to \todo{chemeleon equation} this method proves to be sufficient.

\subsubsection{Gauss-Seidel relaxation}
The most popular smoothing method is Gauss-Seidel relaxation. The Gauss-Seidel scheme for $N_m$ mesh points is
\eq{
    \label{eq:GS}
    u_i\new = u_i -\left(\sum^{N_m}_{j\neq i}L_{ij}u_j-f_i\right)\frac{1}{L_{ii}}\,,
}
where new values of $u$ are used on the right-hand side as they become available. It is usually best to use tile scheme,  making one pass through the mesh updating the even points and another pass updating the odd points.

For nonlinear problems as ours one needs to modify the scheme \eqref{eq:GS} by Newton iteration
\eq{
    u_i\new=u_i-\frac{L_i(u_i)-f_i}{\partial L_i(u_i)/\partial u_i}\,.
}
\section{Initial conditions}
\todo{
https://arxiv.org/pdf/0804.3536.pdf

We want IC such as: $P(k)$, Homogeneity, isotropy -- glass condition, grid condition, gaussianity, $n(k)=1$

from the initial $P(k)$ generate $\hat\delta(k)$ on the mesh,

solve the field equations on the mesh,

calculate the force field $\nabla\phi_V(\mb x)$ on the mesh,

use ZA to get the initial positions and velocities.
    

initial conditions with opposite phases to accelerate convergence \parencite{PhysRevD.93.103519}.

}

\section{Core Cosmology Library}
The Core Cosmology Library (\code{CCL}, \textcite{2019ApJS..242....2C}) is an open source software package written in \code{C}, with a \code{python} interface. The \code{CCL} provides routines to compute basic cosmological observables such as varius distances, power spectra, correlation functions and many others. The accuracy of all quantitites has been extensively tested and compared with many other independent software packages. This allows to establish a well-defined numerical accuracy for each predicted quantity. The \code{CCL} was developed for the needs of the DESC of the LSST which needs very precise predictions for the next generation of cosmological analysis. The author contributed to the development of this library.

We use \code{CCL} for quantities such as linear and non-linear power spectra or growth functions. Internally, \code{CCL} calculates the matter power spectrum  $P(k)$ using various methods including common approximations, by calling external software such as Cosmic Linear Anisotropy Solving System (\code{CLASS}, \textcite{class}), or emulators, such as the CosmicEmu emulator \parencite{Heitmann:2015xma}.

\section{Other methods}
\subsection{TreePM}
The tree code was pioneered by \textcite{1986Natur.324..446B} and uses a hierarchical spatial tree to define localised groups of particles. This has advantage over an equidistant mesh when dealing with highly inhomogeneous systems like stellar dynamics when few cells would contain most of the particles. With the usual oct-tree each cubic cell containing more than some maximum number of particles is split into eight child cells of half their parent's size. This results in a tree-like hierarchy of cubic nodes with the root box, containing all particles, at its bottom. The particles within each of the tree nodes constitute a well-defined and localised group. The potential of a cell is obtained by multipole expansion around some centre of the group of particles (usually center of mass). The gravity at any point is then approximated by summation of potentials from all other cells while considering only cells with sufficiently small opening angle (size of the cell over the distance). If the opening angle is big the same algorithm is applied to the daughter cells.
\subsection{Fast multipole methods}
The fast multipole method of \textcite{1987JCoPh..73..325G} also works with localised particle groups and was originally proposed using a fixed grid but \textcite{2000ApJ...536L..39D} implemnted the method using a tree structure with better results. The method works similarly as the tree code but in addition to expanding the potential at the source positions it also expands it at the sink positions. This dual expansion at both ends of all interactions considerably speeds up the simultaneous computation of gravity for all particles, but brings no advantage over the tree code when computing the force at a single position.

The second part is the interaction phase, when the coefficients of the Taylor series are evaluated for each cell. If the opening angle for a mutual cell-cell interaction is small enough the coefficients for the interactions in each direction are computed.  Otherwise, the interaction is split into up to 8 new interactions by opening the bigger of the two cells. The total computational
costs of this algorithm are dominated by the interaction phase, which only requires $\OO(N_p)$ interactions for the computation of all $N_p$ particle forces. This represents a substantial reduction from the $\OO(N_p^2)$ for direct summation and can greatly imporves simulations of astrophysical problems with inhomogeneous particle distributions.
\section{Alternatives to \nbody\ simulations}
\todo{Boltzmann equation, orbit averaging, Monte-Carlo}

\section{Validation}